{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "TTSInferenceHindi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUtte-UL1r3K"
      },
      "source": [
        "**Hindi TTS Interactive notebook using Tacotron 2 and Waveglow**\n",
        "\n",
        "In this notebook, we will be using our checkpoint file of the training notebook we ran previously and use the WaveGlow decoder to get the final output of Hindi Text to Speech.\n",
        "\n",
        "WaveGlow is a flow-based network capable of generating high-quality speech from mel spectrograms. In its true sense, it is a generative model that generates audio by sampling from a distribution. To use a neural network as a generative model, we take samples from a simple distribution, in our case, a zero mean spherical Gaussian with the same number of dimensions as our desired output, and put those samples through a series of layers that transforms the simple distribution to one which has the desired distribution. In this case, we model the distribution of audio samples conditioned on a mel spectrogram.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPPQDu7an0RR"
      },
      "source": [
        "**Step 1: Mounting Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxZkId54DJCm",
        "outputId": "9c28774c-3e3a-48a4-e062-710c0dc607cd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ03CPl81r3P"
      },
      "source": [
        "**Step 2: Installing dependencies required**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAPdvdWgDCJ3"
      },
      "source": [
        "!pip install hparams\n",
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rWkhnTWoD9I"
      },
      "source": [
        "**Step 3: Migrating to the tacotron 2 folder and changing the tensorflow version to 1.xx**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qRss7qlDOn1",
        "outputId": "2ba0bd8a-fbee-4175-a9ef-b0816d1151fc"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/SSMT/tacotron2\"\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/SSMT/tacotron2\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDyE0EiPoNDC"
      },
      "source": [
        "**Step 4: Importing the necessary packages and files needed to run the inference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSfu6srS1r3P"
      },
      "source": [
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import IPython.display as ipd\n",
        "\n",
        "import sys\n",
        "sys.path.append('waveglow/')\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from hparams import create_hparams\n",
        "from model import Tacotron2\n",
        "from layers import TacotronSTFT, STFT\n",
        "from audio_processing import griffin_lim\n",
        "from train import load_model\n",
        "from text import text_to_sequence\n",
        "from denoiser import Denoiser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIP_YjeYoYw5"
      },
      "source": [
        "**Step 5: Setting up the mel spectrogram plotting function and calling the hyperparamater generator in its default setting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGOzZVIF1r3Q"
      },
      "source": [
        "# This function is used to plot the mel-spectrogram of the generated output and alignment graph\n",
        "def plot_data(data, figsize=(16, 4)):\n",
        "    fig, axes = plt.subplots(1, len(data), figsize=figsize)\n",
        "    for i in range(len(data)):\n",
        "        axes[i].imshow(data[i], aspect='auto', origin='bottom', \n",
        "                       interpolation='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goiKFKn31r3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c56b334-95e1-4613-aaea-e5e09c50dadb"
      },
      "source": [
        "hparams = create_hparams()\n",
        "hparams.sampling_rate = 22050"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IepkxFY1r3R"
      },
      "source": [
        "**Step 6: Load model from checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjhbp29i1r3R"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/SSMT/checkpoints/test12/checkpoint_4400\"\n",
        "model = load_model(hparams)\n",
        "model.load_state_dict(torch.load(checkpoint_path)['state_dict'])\n",
        "_ = model.cuda().eval().half()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O1_qu2x1r3S"
      },
      "source": [
        "**`Step 7: Load WaveGlow for mel2audio synthesis and denoiser`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_niMN9lT1r3S"
      },
      "source": [
        "waveglow_path = '/content/drive/MyDrive/SSMT/waveglow_256channels_universal_v5.pt'\n",
        "waveglow = torch.load(waveglow_path)['model']\n",
        "waveglow.cuda().eval().half()\n",
        "for k in waveglow.convinv:\n",
        "    k.float()\n",
        "denoiser = Denoiser(waveglow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q84WjRp1r3S"
      },
      "source": [
        "**Step 8: Prepare text input**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oFSrTPs1r3S"
      },
      "source": [
        "text = \"मैं बाज़ार जाता हूँ \"\n",
        "sequence = np.array(text_to_sequence(text, ['transliteration_cleaners']))[None, :]\n",
        "sequence = torch.autograd.Variable(\n",
        "    torch.from_numpy(sequence)).cuda().long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkqe3MMC1r3S"
      },
      "source": [
        "**Step 9: Decode text input and plot results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vbBrivqX1r3T"
      },
      "source": [
        "mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n",
        "plot_data((mel_outputs_postnet.float().data.cpu().numpy()[0],\n",
        "           alignments.float().data.cpu().numpy()[0].T))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5dJQPD6o49p"
      },
      "source": [
        "***A short summary about melspectrograms:***\n",
        "\n",
        "Sound is heard as a result of the variation of pressure with time. However, speech signals are complex entities and a simple pressure variation does not capture enough information for the deep learning model to be trained. Hence in short, a melspectrogram, is a graph which plots three quanitites - Time on the X axis, Frequency on the Y axis and the colors represent the loudness of the sound. \n",
        "\n",
        "The alignment graph seen above is a simple representation of the trajectory of the final output compared to its initial text input\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85LAd9LA1r3T"
      },
      "source": [
        "**Step 10: Synthesize audio from spectrogram using WaveGlow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sG6ZKYS1r3T"
      },
      "source": [
        "with torch.no_grad():\n",
        "    audio = waveglow.infer(mel_outputs_postnet, sigma=0.666)\n",
        "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4O66_Q-1r3T"
      },
      "source": [
        "**Step 11: (Optional) Remove WaveGlow bias**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaS1b6RX1r3U"
      },
      "source": [
        "audio_denoised = denoiser(audio, strength=0.01)[:, 0]\n",
        "ipd.Audio(audio_denoised.cpu().numpy(), rate=hparams.sampling_rate) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmcwrypmqZio"
      },
      "source": [
        "**References**\n",
        "\n",
        "* \"Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions\" - Shen et al, ICASSP 2018\n",
        "\n",
        "* \"WaveGlow: A Flow-based Generative Network for Speech Synthesis\" - Prenger et al, ICASSP 2019\n",
        "\n",
        "* Indic TTS Databasee - IIT Madras : https://www.iitm.ac.in/donlab/tts/index.php\n",
        "\n",
        "* Blog on English TTS Using Tacotron 2 and WaveGlow by Nvidia: https://developer.nvidia.com/blog/generate-natural-sounding-speech-from-text-in-real-time/\n",
        "\n",
        "\n"
      ]
    }
  ]
}